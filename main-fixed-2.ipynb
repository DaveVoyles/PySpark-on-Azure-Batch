{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azure.storage.blob import BlockBlobService\nfrom pyspark            import SparkConf,    SparkContext\nfrom pyspark.sql        import SparkSession, SQLContext\nfrom pyspark.sql.types  import *\n\nstorageAccountName = \"dvbatch\"\nstorageKey         = \"d/xdWGVlp4DYi7JCvjEuYW/OaeSBEupMpG/5SlyE7CheMA0s1rHAByjxQ3zSemgvCI70BcyDDpT5s9K1BVMO3g==\"\ncontainerName      = \"output\"\nfile               = \"fixed-width-2.txt\"\n\n# Establish connection with the blob storage account\nblobService = BlockBlobService(account_name=storageAccountName,\n                               account_key =storageKey\n                               )",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create spark session\nspark = SparkSession.builder.master(\"local\").appName(\"fixed-width\"                          )\\\n                                            .config(\"spark.some.config.option\", \"some-value\")\\\n                                            .getOrCreate()\n# Read in fixed-width text file\ndf = spark.read.option(\"header\"     , \"false\")\\\n               .option(\"inferSchema\", \"false\")\\\n               .text(file                    )",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Generate a schema for each of the columns\nschema = StructType([StructField('col1', IntegerType(), True),\n                     StructField('col2', IntegerType(), True),\n                     StructField('col3', IntegerType(), True)])",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Take the fixed width file and split into 3 distinct columns\nsorted_df = df.select(\n    df.value.substr(1, 4).alias('col1'),\n    df.value.substr(5, 4).alias('col2'),\n    df.value.substr(8, 4).alias('col3'),\n)",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Cast each column to int\nfrom pyspark.sql import functions as f\n\ncasting  = [(f.col(col_name).cast(\"int\"))\n             .name(col_name) \n               for col_name in sorted_df.columns]\n\nsorted_df = sorted_df.select(casting)",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Show results\nsorted_df.show()\nsorted_df.printSchema()",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "+----+----+----+\n|col1|col2|col3|\n+----+----+----+\n|1234|5678|8135|\n|1234|5678|8135|\n|1234|5678|8135|\n|1234|5678|8135|\n|1234|5678|8135|\n|1234|5678|8135|\n|1234|5678|8135|\n|1234|5678|8135|\n|1234|5678|8135|\n|1234|5678|8135|\n|1234|5678|8135|\n|1234|5678|8135|\n|1234|5678|8135|\n+----+----+----+\n\nroot\n |-- col1: integer (nullable = true)\n |-- col2: integer (nullable = true)\n |-- col3: integer (nullable = true)\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Apply the schmea to dataframe\nnew_df = spark.createDataFrame(sorted_df.rdd,schema).show()",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "+----+----+----+\n|col1|col2|col3|\n+----+----+----+\n|1234|5678|8135|\n|1234|5678|8135|\n|1234|5678|8135|\n|1234|5678|8135|\n|1234|5678|8135|\n|1234|5678|8135|\n|1234|5678|8135|\n|1234|5678|8135|\n|1234|5678|8135|\n|1234|5678|8135|\n|1234|5678|8135|\n|1234|5678|8135|\n|1234|5678|8135|\n+----+----+----+\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "\"\"\"\nTime stamp to file path to prevent saving over orignial file.\n\"\"\"\ndef createTimeStamp(): \n    from datetime import datetime\n\n    # datetime object containing current date and time\n    now = datetime.now()\n\n    # /dd-mm-YY_H:M\n    dt_string = now.strftime(\"/%d-%m-%Y_%H-%M\")    \n\n    return dt_string",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create an output folder with a timestamp to prevent overwriting files\noutput_dir   = \"output\" + createTimeStamp() \nprint(output_dir)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Make directory and write files to it\nimport os\nfrom   os import path\n\ntry:\n    sorted_df.write.parquet(output_dir)\n    files_in_dir = output_dir +\"/*\"\n    \nexcept FileExistsError:\n    print(\"Path exists -- skipping\")\n    print(output_dir)\n    pass",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Print files we just saved with Spark\nimport glob\nprint(glob.glob(files_in_dir))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Write/upload files to blob storage\nfor file in glob.glob(files_in_dir):\n    print(file)\n    blobService.create_blob_from_path(containerName, file, file)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
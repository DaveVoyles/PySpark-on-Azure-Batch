{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azure.storage.blob import BlockBlobService\nfrom pyspark            import SparkConf,    SparkContext\nfrom pyspark.sql        import SparkSession, SQLContext\nfrom pyspark.sql.types  import *\n\nstorageAccountName = \"dvbatch\"\nstorageKey         = \"d/xdWGVlp4DYi7JCvjEuYW/OaeSBEupMpG/5SlyE7CheMA0s1rHAByjxQ3zSemgvCI70BcyDDpT5s9K1BVMO3g==\"\ncontainerName      = \"output\"\nfile               = \"fixed-width-1.txt\"\n\n# Establish connection with the blob storage account\nblobService = BlockBlobService(account_name=storageAccountName,\n                               account_key =storageKey\n                               )",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from pyspark.context import SparkContext\n\nsc       = SparkContext()\nspark    = SparkSession(sc)\nrdd_df   = sc.textFile(file)",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "firstRow = rdd_df.first()\nrdd_df   = rdd_df.filter(lambda row:row != firstRow)",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Generate a schema for each of the columns\nschema = StructType([StructField('Entry'      , IntegerType(), True),\n                     StructField('Per'        , IntegerType(), True),\n                     StructField('Account'    ,    LongType(), True),\n                     StructField('Description',  StringType(), True)])",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#from pyspark.sql.types import *\n#schema = StructType([StructField('col1', IntegerType(), True),\n#                     StructField('col2', IntegerType(), True),\n#                     StructField('col3', IntegerType(), True)])\n#df=spark.createDataFrame(\n#spark.sparkContext.textFile(\"fixed-width-1.txt\").\\\n#map(lambda x:(int(x[0:4]),int(x[4:8]),int(x[8:12]))),schema)\n#df.printSchema()",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sqlContext  = SQLContext(sc)\ndf_from_rdd = sqlContext.createDataFrame(rdd_df, schema)",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# ERROR: AttributeError: 'PipelinedRDD' object has no attribute 'value'\n# Take the fixed width file and split into 3 distinct columns\nsorted_df = df_from_rdd.select(\n    df_from_rdd.value.substr( 1,  6).alias('Entry'      ),\n    df_from_rdd.value.substr( 8,  3).alias('Per'        ),\n    df_from_rdd.value.substr(12, 11).alias('GL Account' ),\n    df_from_rdd.value.substr(24, 11).alias('Description'),\n)",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DataFrame' object has no attribute 'value'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-d0687540f9f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Take the fixed width file and split into 3 distinct columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m sorted_df = df_from_rdd.select(\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdf_from_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubstr\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Entry'\u001b[0m      \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdf_from_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubstr\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Per'\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdf_from_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GL Account'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1301\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1302\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'value'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Show results\nsorted_df.show()\nsorted_df.printSchema()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Cast each column to int\n#from pyspark.sql import functions as f\n\n#casting  = [(f.col(Description).cast(\"string\"))\n#            .name(col_name) \n#               for col_name in sorted_df.columns]\n\n#sorted_df = sorted_df.select(casting)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "\"\"\"\nTime stamp to file path to prevent saving over orignial file.\n\"\"\"\ndef createTimeStamp(): \n    from datetime import datetime\n\n    # datetime object containing current date and time\n    now = datetime.now()\n\n    # /dd-mm-YY_H:M\n    dt_string = now.strftime(\"/%d-%m-%Y_%H-%M\")    \n\n    return dt_string",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create an output folder with a timestamp to prevent overwriting files\n#output_dir   = \"output\" + createTimeStamp() \n#print(output_dir)\n\noutput_dir = 'output/rdd/'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Make directory and write files to it\nimport os\nfrom   os import path\n\ntry:\n    df_from_rdd.write.parquet(output_dir)\n    files_in_dir = output_dir +\"/*\"\n    \nexcept FileExistsError:\n    print(\"Path exists -- skipping\")\n    print(output_dir)\n    pass",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Print files we just saved with Spark\nimport glob\nprint(glob.glob(files_in_dir))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Write/upload files to blob storage\nfor file in glob.glob(files_in_dir):\n    print(file)\n    blobService.create_blob_from_path(containerName, file, file)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}